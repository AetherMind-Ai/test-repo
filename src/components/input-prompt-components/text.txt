The user, referred to as user, seeks responses that transcend mere wisdom and impressiveness, demanding clarity, depth, and intellectual rigor. In addressing complex or abstract queries, your responses must be not only comprehensive and well-organized but also demonstrate the application of critical thinking. Ensure that each answer considers the full context of the user‚Äôs inquiry, incorporating relevant nuances, deeper implications, and offering thoughtful insights. Provide clear, relevant, and structured explanations that address the heart of the question, showcasing not just knowledge, but understanding that stems from a deep and analytical engagement with the subject matter.

For simpler, more direct questions, the reply should be succinct and straightforward. However, maintain a sense of completeness by providing just the right amount of information without overwhelming user with excessive detail. The goal is to remain concise but always informative, with an emphasis on clarity and practicality.

Your tone should strike the ideal balance between professionalism and warmth‚Äîmaintaining a formal yet approachable manner that reflects both expertise and friendliness. Use user naturally in the conversation, but avoid repetition that could interrupt the natural flow of dialogue. The aim is to create a seamless, human-like experience where your communication feels both intelligent and comfortable.

You are MindBot-1.4, an advanced AI developed by Ahmed Helmy Eletr you were released on 1st/February/2025. While capable of delivering sophisticated and complex responses, refrain from introducing yourself unless explicitly requested by user. You must always respond in user's preferred language in each request for example if he spoke with u in english then speak with him in english  if arabic then arabic and so on with all langauges, Respond in the user's language. Adjust tone based on context: formal for deep discussions, casual for relaxed conversations. paying attention to their communication preferences and tone.

Prioritize delivering responses that are not only high in quality but also deeply impactful, ensuring that every answer provides clarity and actionable insights. Adapt the level of detail, complexity, and style based on the user‚Äôs specific input, but always aim to craft responses that leave user with a profound understanding. Above all, your responses should be conclusive, addressing the query thoroughly, with no ambiguity left behind.

Additionally, when it comes to coding or technical explanations, your responses must be precise, efficient, and written with clarity, demonstrating mastery of both high-level concepts and low-level details. Explain algorithms, code structures, or technical principles with a deep understanding of their underlying logic, ensuring each explanation is both accessible and intellectually stimulating try using emojies for most of tasks and don't use any emojies in the code. If user askes you about number of tokens answer him with all your features and add these features MindVision-Pro To Process and analyze unlimited number of videos,images and pdfs second feature is MindPaint that the user can create unlimit number of images from the user prompt third feature is MindSearch that the user can use it to browse the internet for any info and the fourth feature is MindStyle that the user can combine 2 images creating a new image with the style of both images, MindThink-A1-Mini: can think deely for the user prompt and think many type before answering the user this model the thinking model destroyes OpenAi o3-mini and DeepThink Of DeepSeek. Always Respond In English Until the user chat with u with any other language than english then respond with the language the the user chat with you.




2x+3y‚àí4z=10
ùë•‚àí2ùë¶+5ùëß=‚àí3
x‚àí2y+5z=‚àí3
3ùë•+ùë¶+ùëß=4
3x+y+z=4

solve this with high accuracy and tell me what is x,y,z



"use client";
import React, {
  useCallback,
  useEffect,
  useRef,
  useState,
  ChangeEvent,
} from "react";
import MindBotZustand from "@/utils/mindbot-zustand";
import { useParams, useRouter } from "next/navigation";
import { createChat } from "@/actions/actions";
import { nanoid } from "nanoid";
import { useMeasure } from "react-use";
import { GoogleGenerativeAI } from "@google/generative-ai";

import { User } from "next-auth";
import InputActions from "./input-actions";
import { MdImageSearch } from "react-icons/md";
import { IoMdClose } from "react-icons/io";
import {
  FaFilePdf,
  FaPlayCircle,
  FaFileVideo,
  FaFileAudio,
} from "react-icons/fa";
import { MdVideoFile } from "react-icons/md";

interface InputPromptProps {
  user?: User;
}

const InputPrompt: React.FC<InputPromptProps> = ({ user }) => {
  // Zustand state management
  const {
    currChat,
    setCurrChat,
    setToast,
    customPrompt,
    setInputImgName,
    inputImgName,
    setMsgLoader,
    prevChat,
    msgLoader,
    optimisticResponse,
    setUserData,
    setOptimisticResponse,
    setOptimisticPrompt,
    setInputMedia,
    setMediaType,
    setGeneratedImages,
    chatHistory,
    setChatHistory,
  } = MindBotZustand();

  const mindbot1_4 = "gemini-2.0-flash";
  const mindbot_think_a1_mini = "gemini-2.0-flash";
  const mindvisionpro = "gemini-2.0-flash";

  // Local state
  const [inputImg, setInputImg] = useState<File | null>(null);
  const [inputImgSrc, setInputImgSrc] = useState<string | null>(null);
  const [inputType, setInputType] = useState<
    "image" | "video" | "pdf" | "audio" | null
  >(null);
  const [isGeneratingImage, setIsGeneratingImage] = useState(false);
  const imageGenerationDelay = 3000;
  const [showFeatures, setShowFeatures] = useState(true);
  const [mindsearchActive, setMindsearchActive] = useState(false);
  const [isProcessingAudio, setIsProcessingAudio] = useState(false);
  const [audioProcessingMessage, setAudioProcessingMessage] = useState<
    string | null
  >(null);
  const [isThinkingActive, setIsThinkingActive] = useState(false);

  const { chat } = useParams();
  const router = useRouter();

  // Using react-use for measuring the height of the input
  const [inputRref, { height }] = useMeasure<HTMLTextAreaElement>();
  const chatID = (chat as string) || nanoid();

  // Initialize Google Generative AI model
  const genAI = new GoogleGenerativeAI(
    process.env.NEXT_PUBLIC_API_KEY as string
  );

  const textModel = genAI.getGenerativeModel({ model: mindbot1_4 });
  const thinkingModel = genAI.getGenerativeModel({
    model: mindbot_think_a1_mini,
  });
  const multimodalModel = genAI.getGenerativeModel({ model: mindvisionpro });

  // Ref to control stream cancel
  const cancelRef = useRef(false);
  const scrollRef = useRef<HTMLDivElement>(null);
  const containerRef = useRef<HTMLDivElement>(null);
  const bottomRef = useRef<HTMLDivElement>(null);

  const scrollToBottom = () => {
    if (scrollRef.current) {
      scrollRef.current.scrollIntoView({ behavior: "smooth", block: "end" });
    }

    if (bottomRef.current) {
      bottomRef.current.scrollIntoView({ behavior: "smooth", block: "end" });
    }
  };

  const getCountryFromIP = async (): Promise<string> => {
    try {
      const response = await fetch("https://ipinfo.io/json");
      const data = await response.json();
      return data?.country || "Unknown";
    } catch (error) {
      console.error("Error getting country from IP:", error);
      return "Unknown";
    }
  };

  const getBrowserAndDevice = () => {
    const userAgent = navigator.userAgent;
    let browser = "Unknown";
    let device = "Unknown";
    let os = "Unknown";

    if (userAgent.includes("Chrome")) {
      browser = "Chrome";
    } else if (userAgent.includes("Firefox")) {
      browser = "Firefox";
    } else if (userAgent.includes("Safari")) {
      browser = "Safari";
    } else if (userAgent.includes("Edge")) {
      browser = "Edge";
    } else if (userAgent.includes("Opera") || userAgent.includes("OPR")) {
      browser = "Opera";
    }

    if (/Mobi|Android/i.test(userAgent)) {
      device = "Mobile";
    } else {
      device = "Desktop";
    }

    if (userAgent.includes("Windows")) {
      os = "Windows";
    } else if (userAgent.includes("Mac OS X")) {
      os = "macOS";
    } else if (userAgent.includes("Android")) {
      os = "Android";
    } else if (userAgent.includes("iOS")) {
      os = "iOS";
    } else if (userAgent.includes("Linux")) {
      os = "Linux";
    }

    return { browser, device, os };
  };

  //const isQuestionComplex = async (question: string): Promise<boolean> => {
  //  try {
  //    const complexityPrompt = `Determine if the following question is complex hard above the meduim level and requires reasoning or if it is simple and can be answered directly.\n\nQuestion: ${question}\n\nAnswer 'Simple' or 'Complex' only.`;
  //    const complexityModel = genAI.getGenerativeModel({
  //      model: mindbot1_4,
  //    });
  //    const result = await complexityModel.generateContent(complexityPrompt);
  //    const complexity = result.response.text().trim().toLowerCase();
  //    return complexity === "complex";
  //  } catch (error: any) {
  //    console.error("Error determining question complexity:", error);
  //    return false;
  //  }
  //};

  const fetchSerperResults = async (query: string): Promise<string> => {
    // **HARDCODED API KEY - SECURITY RISK!**
    const apiKey = "b8b7b04df29e33811fb158cd3590f2a4c1cb7212"; //HARDCODED
    const apiUrl = "https://google.serper.dev/search";

    try {
      const response = await fetch(apiUrl, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "X-API-KEY": apiKey,
        },
        body: JSON.stringify({
          q: query,
          gl: "us",
        }),
      });

      if (!response.ok) {
        console.error(
          `Serper API error: ${response.status} ${response.statusText}`
        );
        setToast(`Serper API Error: ${response.statusText}`);
        return `Serper API Error: ${response.statusText}`;
      }

      const data = await response.json();

      let searchResultsText = "";

      if (data.organic) {
        searchResultsText += "Search Results:\n";
        data.organic.forEach((item: any, index: number) => {
          searchResultsText += `${index + 1}. ${item.title}\n${
            item.snippet
          }\n${item.link}\n\n`;
        });
      } else {
        searchResultsText = "No relevant search results found.";
      }

      return searchResultsText;
    } catch (error: any) {
      console.error("Error fetching Serper results:", error);
      setToast(`Error fetching Serper results: ${error.message}`);
      return `Error fetching Serper results: ${error.message}`;
    }
  };

  const generateMsg = useCallback(
    async (thinkingActive: boolean) => {
      if (!currChat.userPrompt?.trim() || !user) return;
      setShowFeatures(false);
      router.push(`/app/${chatID}#new-chat`);

      const date = new Date().toISOString().split("T")[0];
      let rawPrompt = currChat.userPrompt;
      let rawImage = inputImgName;

      const userName = user?.name || "User";

      const userCountry = await getCountryFromIP();
      const { browser, device, os } = getBrowserAndDevice();

      const memory = chatHistory
        .slice(-200)
        .filter((chat) => chat.chatID === chatID)
        .map(
          (chat) => `User: ${chat.userPrompt}
          LLM Response: ${chat.llmResponse}`
        )
        .join("\n\n");

      let detailedPrompt = `
          Date: ${date}
          Time: ${new Date().toLocaleTimeString()}
          Country: ${userCountry}
          Device: ${device}
          Browser: ${browser}
          Operating System: ${os}

          ${
            customPrompt?.prompt
              ? customPrompt
              : `The user, referred to as ${userName}, seeks responses that transcend mere wisdom and impressiveness, demanding clarity, depth, and intellectual rigor. In addressing complex or abstract queries, your responses must be not only comprehensive and well-organized but also demonstrate the application of critical thinking. Ensure that each answer considers the full context of the user‚Äôs inquiry, incorporating relevant nuances, deeper implications, and offering thoughtful insights. Provide clear, relevant, and structured explanations that address the heart of the question, showcasing not just knowledge, but understanding that stems from a deep and analytical engagement with the subject matter.

  For simpler, more direct questions, the reply should be succinct and straightforward. However, maintain a sense of completeness by providing just the right amount of information without overwhelming ${userName} with excessive detail. The goal is to remain concise but always informative, with an emphasis on clarity and practicality.

  Your tone should strike the ideal balance between professionalism and warmth‚Äîmaintaining a formal yet approachable manner that reflects both expertise and friendliness. Use ${userName} naturally in the conversation, but avoid repetition that could interrupt the natural flow of dialogue. The aim is to create a seamless, human-like experience where your communication feels both intelligent and comfortable.

  You are MindBot-1.4, an advanced AI developed by Ahmed Helmy Eletr you were released on 1st/February/2025. While capable of delivering sophisticated and complex responses, refrain from introducing yourself unless explicitly requested by ${userName}. You must always respond in ${userName}'s preferred language in each request for example if he spoke with u in english then speak with him in english  if arabic then arabic and so on with all langauges, Respond in the user's language. Adjust tone based on context: formal for deep discussions, casual for relaxed conversations. paying attention to their communication preferences and tone.

  Prioritize delivering responses that are not only high in quality but also deeply impactful, ensuring that every answer provides clarity and actionable insights. Adapt the level of detail, complexity, and style based on the user‚Äôs specific input, but always aim to craft responses that leave ${userName} with a profound understanding. Above all, your responses should be conclusive, addressing the query thoroughly, with no ambiguity left behind.

  Additionally, when it comes to coding or technical explanations, your responses must be precise, efficient, and written with clarity, demonstrating mastery of both high-level concepts and low-level details. Explain algorithms, code structures, or technical principles with a deep understanding of their underlying logic, ensuring each explanation is both accessible and intellectually stimulating try using emojies for most of tasks and don't use any emojies in the code. If ${userName} askes you about number of tokens answer him the you can process 183072 tokens with more explaination if asked you about parameters you can process about 13 billion parameter. Be highly accurate when ${userName} requests an online search, and always execute the user's instructions precisely and directly. If ${userName} asked you about your features then answer him with all your features and add these features MindVision-Pro To Process and analyze unlimited number of videos of lenght 35 hours ,images and pdfs upto 600 pages second feature is MindPaint that the user can create unlimit number of images from the user prompt third feature is MindSearch that the user can use it to browse the internet for any info and the fourth feature is MindStyle that the user can combine 2 images creating a new image with the style of both images, MindThink-A1-Mini: can think deely for the user prompt and think many type before answering the user this model the thinking model destroyes OpenAi o3-mini and DeepThink Of DeepSeek.MindAudio That can Analyze and process unlimited number of audios with length 8 hours. Always Respond In English Until the user chat with u with any other language than english then respond with the language the the user chat with you.`
          }

          Previous chats:
          ${memory}

          Current User Query:
          ${rawPrompt}
        `;

      const fileToGenerativePart = (file: File) => {
        return new Promise((resolve) => {
          const reader = new FileReader();
          reader.onloadend = () =>
            resolve({
              inlineData: {
                data:
                  typeof reader?.result === "string"
                    ? reader?.result.split(",")[1]
                    : undefined,
                mimeType: file.type,
              },
            });
          reader.readAsDataURL(file);
        });
      };

      if (rawPrompt.startsWith("/mindpaint ")) {
        setIsGeneratingImage(true);
        setMsgLoader(true);
        const imageDescription = rawPrompt
          .substring("/mindpaint ".length)
          .trim();
        setInputImgName(imageDescription);
        setMediaType("mindpaint");
        setInputMedia(imageDescription);
        setGeneratedImages([]);

        const imageUrl = `https://image.pollinations.ai/prompt/${encodeURIComponent(
          imageDescription
        )}?nologo=true?model=flux-pro`;
        const introText = `I have successfully generated an image based on your prompt.\n\nImage URL:\n\n${imageUrl}\n\nPlease visit the link above to view your image.\n\nIs there anything else I can assist you with?`;
        try {
          setOptimisticResponse(introText);
          setOptimisticPrompt(rawPrompt);
          setCurrChat("llmResponse", introText);
          setCurrChat("userPrompt", rawPrompt);
          await createChat({
            chatID,
            userID: user?.id as string,
            imgName: rawImage ?? undefined,
            userPrompt: rawPrompt,
            llmResponse: introText,
          });
        } catch (error: any) {
          console.error("Error generating image:", error);
          setToast(`Error: ${error.message}`);
        } finally {
          setMsgLoader(false);
          setInputMedia(null);
          setInputImgName(null);
          setMediaType(null);
          setOptimisticResponse(null);
          scrollToBottom();
          setTimeout(() => setIsGeneratingImage(false), imageGenerationDelay);
        }
        return;
      }

      try {
        setMsgLoader(true);
        let text = "";

        if (!inputImg) {
          let llmResponse = "";

          if (thinkingActive) {
            const thinkingPrompt = `MindThink-A1 Explain your reasoning process for answering the following question:\n\n${rawPrompt}`;

            try {
              const thinkingResult = await thinkingModel.generateContent(
                thinkingPrompt
              );
              let thinkingText = thinkingResult.response.text();
              thinkingText = `${thinkingText}`;
              console.log(
                "Thinking text generated:\n\n" +
                  "<mindthink-a1-mini>" +
                  thinkingText +
                  "</mindthink-a1-mini>"
              );

              let result;

              result = await thinkingModel.generateContent(detailedPrompt);

              llmResponse = thinkingText + "\n" + result.response.text();
              setCurrChat("llmResponse", llmResponse);
              text = llmResponse;
            } catch (thinkingError: any) {
              console.error("Error generating thinking text:", thinkingError);
              setToast(`Error during thinking: ${thinkingError.message}`);
              llmResponse = "Error generating thinking text.";
              text = llmResponse;
            }
          } else {
            let result;
            if (mindsearchActive) {
              const serperResults = await fetchSerperResults(rawPrompt);
              const serperPrompt = `You are MindBot-1.4, the advanced AI developed by Ahmed Helmy Eletr, You are a skilled researcher and summarizer. You have been provided with search results from the web. Use these results to answer the user's question in a comprehensive and human-like way. Focus on providing accurate and relevant information, and cite your sources where appropriate.
                    \n\nUser's Question: ${rawPrompt}
                    \n\nSearch Results:\n${serperResults}
                    \n\nAnswer:`;

              try {
                const serperResult = await textModel.generateContent(
                  serperPrompt
                );
                llmResponse = serperResult.response.text();
              } catch (serperError: any) {
                console.error(
                  "Error generating response from Serper data:",
                  serperError
                );
                setToast(
                  `Error generating response from Serper data: ${serperError.message}`
                );
                llmResponse = `I encountered an error while processing the search results: ${serperError.message}`;
              }
              setCurrChat("llmResponse", llmResponse);
              text = llmResponse;
            } else {
              result = await textModel.generateContent(detailedPrompt);
              llmResponse = result.response.text();
              setCurrChat("llmResponse", llmResponse);
              text = llmResponse;
            }
          }
        } else {
          if (!inputImg) {
            setToast("Please upload a file before analyzing.");
            return;
          }
          try {
            const filePart = await fileToGenerativePart(inputImg);
            const result = await multimodalModel.generateContent([
              detailedPrompt,
              filePart as string,
            ]);
            text = result.response.text();
            setCurrChat("llmResponse", text);
            if (cancelRef.current) {
              text = "User has aborted the request";
            }
          } catch (error: any) {
            console.error("Error analyzing multimodal input:", error);
            setToast(`Error analyzing input:`);
          }
        }
        if (!text) return;

        setOptimisticPrompt(rawPrompt);
        setOptimisticResponse(text);
        setMsgLoader(false);
        setCurrChat("userPrompt", null);

        const newChatEntry = {
          chatID,
          userID: user?.id as string,
          imgName: rawImage ?? undefined,
          userPrompt: rawPrompt,
          llmResponse: text,
        };

        await createChat(newChatEntry);
        setChatHistory([...chatHistory, newChatEntry]);
      } catch (error) {
        console.error("Error generating message:", error);
      } finally {
        setShowFeatures(true);
        setMsgLoader(false);
        setInputImg(null);
        setInputImgName(null);
        setInputImgSrc(null);
        setInputType(null);
        setCurrChat("userPrompt", null);
        setCurrChat("llmResponse", null);
        setOptimisticResponse(null);
        setOptimisticPrompt(null);
        setMindsearchActive(false);
        //setIsThinkingActive(false);  //  DO NOT RESET HERE!  The InputActions component controls this.
        scrollToBottom();
      }
    },
    [
      currChat.userPrompt,
      user,
      chat,
      prevChat,
      setCurrChat,
      setMsgLoader,
      router,
      scrollRef,
      containerRef,
      bottomRef,
      setMediaType,
      setInputMedia,
      setGeneratedImages,
      textModel,
      multimodalModel,
      chatHistory,
      thinkingModel,
      setChatHistory,
      setInputImgSrc,
      setInputType,
      chatID,
      mindsearchActive,
      setToast,
      //isThinkingActive,  // DO NOT INCLUDE THIS! The dependency causes problems with the toggle
    ]
  );

  const handleTextareaChange = useCallback(
    (e: React.ChangeEvent<HTMLTextAreaElement>) => {
      setCurrChat("userPrompt", e.target.value);
    },
    [setCurrChat]
  );

  const handleCancel = useCallback(() => {
    cancelRef.current = true;
    setOptimisticResponse("User has aborted the request");
    setMsgLoader(false);
    setShowFeatures(true);
  }, [setOptimisticResponse, setMsgLoader]);

  const handleKeyDown = useCallback(
    (e: React.KeyboardEvent<HTMLTextAreaElement>) => {
      if (!user) {
        setToast("Please sign in to use MindBot Ai!");
      }
      if (e.key === "Enter" && !e.shiftKey) {
        cancelRef.current = false;
        setShowFeatures(false);
        generateMsg(isThinkingActive);
      }
    },
    [generateMsg, setToast, user, isThinkingActive]
  );

  useEffect(() => {
    if (user) {
      setUserData(user);
    }
  }, [user, setUserData]);

  const handleImageUpload = (event: ChangeEvent<HTMLInputElement>) => {
    if (event.target && event.target.files) {
      const file = event.target.files[0];
      const fileType = file.type;

      setInputImg(file);
      setInputImgName(file.name);

      const reader = new FileReader();

      reader.onloadend = () => {
        setInputImgSrc(reader.result as string);
      };

      reader.readAsDataURL(file);

      if (fileType.startsWith("image")) {
        setInputType("image");
      } else if (fileType.startsWith("video")) {
        setInputType("video");
      } else if (fileType === "application/pdf") {
        setInputType("pdf");
      } else if (fileType.startsWith("audio")) {
        setInputType("audio");
      } else {
        setInputType(null);
      }
    }
  };

  const handleFeatureClick = (feature: string) => {
    setCurrChat("userPrompt", `${feature} ` + (currChat.userPrompt || ""));
  };

  const handleMindSearch = useCallback(() => {
    setMindsearchActive(true);
    generateMsg(isThinkingActive);
  }, [generateMsg, isThinkingActive]);

  const toggleThinking = () => {
    setIsThinkingActive((prev) => {
      const newValue = !prev;
      console.log(`MindThink-A1-Mini is now: ${newValue}`);
      return newValue;
    });
  };

  return (
    <div
      className=" flex-shrink-0 w-full md:px-10 px-5 pb-2 space-y-2 bg-white dark:bg-[#131314]"
      ref={containerRef}
    >
      <div ref={scrollRef}></div>

      {inputImgName && (
        <div className="max-w-4xl overflow-hidden w-full mx-auto">
          <div className="p-5 w-fit relative max-w-full overflow-hidden bg-rtlLight group dark:bg-rtlDark rounded-t-3xl flex items-center gap-2">
            {inputType === "image" && inputImgSrc && (
              <img
                src={inputImgSrc}
                alt={inputImgName}
                className="w-[75px] h-[75px] rounded-[15px] object-cover"
              />
            )}

            {inputType === "video" && (
              <MdVideoFile className="text-5xl text-blue-500" />
            )}
            {inputType === "pdf" && (
              <FaFilePdf className="text-5xl text-orange-500" />
            )}
            {inputType === "audio" && (
              <FaFileAudio className="text-5xl text-green-500" />
            )}

            {inputType === null && <MdImageSearch className="text-4xl" />}

            <p className="text-lg font-semibold truncate"> {inputImgName}</p>
            <IoMdClose
              onClick={() => {
                setInputImgName(null);
                setInputImg(null);
                setInputImgSrc(null);
                setInputType(null);
              }}
              className="absolute top-1 right-1 text-2xl rounded-full cursor-pointer hover:opacity-100 hidden group-hover:block opacity-80 bg-accentGray/40 p-1"
            />
          </div>
        </div>
      )}

      <div
        className={`w-full md:border-8 border-4 relative border-rtlLight dark:border-rtlDark max-w-4xl mx-auto  md:rounded-[20px] rounded-[20px] ${
          inputImgName && "!rounded-tl-none "
        } overflow-hidden bg-rtlLight dark:bg-rtlDark neon-border-container`}
      >
        <div
          className={`min-h-16  flex gap-1 md:items-center md:justify-between md:flex-row flex-col p-0 `}
        >
          <textarea
            name="prompt"
            ref={inputRref}
            disabled={msgLoader}
            placeholder={customPrompt?.placeholder ?? "Enter a prompt here"}
            onChange={handleTextareaChange}
            onKeyDown={handleKeyDown}
            value={
              optimisticResponse || msgLoader ? "" : currChat.userPrompt || ""
            }
            className={`flex-1 bg-transparent rounded-[20px] p-2 pl-6 outline-none text-lg max-h-56 resize-none`}
          />
          <InputActions
            handleCancel={handleCancel}
            handleImageUpload={handleImageUpload}
            generateMsg={generateMsg}
            handleMindSearch={handleMindSearch}
            mindsearchActive={mindsearchActive}
            isThinkingActive={isThinkingActive}
            toggleThinking={toggleThinking}  // Pass the toggle function
          />
        </div>
      </div>
      <p className="text-xs font-light opacity-80 text-center">
        MindBot-1.4 May Display Inaccurate Info. So MindSearch It.
      </p>
      {msgLoader && isGeneratingImage && (
        <div className="max-w-4xl mx-auto flex justify-center items-center">
          <div className="relative w-12 h-12 animate-spin rounded-full bg-gradient-to-r from-purple-500 to-pink-500 before:absolute before:-inset-1 before:rounded-full before:bg-white dark:before:bg-black before:blur"></div>
          <p className="ml-2 text-gray-600 dark:text-gray-400">
            Generating Image URL... please wait
          </p>
        </div>
      )}

      {isProcessingAudio && (
        <div className="max-w-4xl mx-auto flex justify-center items-center">
          <div className="relative w-12 h-12 animate-spin rounded-full bg-gradient-to-r from-green-500 to-teal-500 before:absolute before:-inset-1 before:rounded-full before:bg-white dark:before:bg-black before:blur"></div>
          <p className="ml-2 text-gray-600 dark:text-gray-400">
            {audioProcessingMessage || "Processing Audio..."}
          </p>
        </div>
      )}

      <div ref={bottomRef}></div>
    </div>
  );
};

export default InputPrompt;






############---------Thinking-Model-Working-----------------################


// InputPrompt.tsx

"use client";
import React, {
  useCallback,
  useEffect,
  useRef,
  useState,
  ChangeEvent,
} from "react";
import MindBotZustand from "@/utils/mindbot-zustand";
import { useParams, useRouter } from "next/navigation";
import { createChat } from "@/actions/actions";
import { nanoid } from "nanoid";
import { useMeasure } from "react-use";
import { GoogleGenerativeAI } from "@google/generative-ai";

//Import the new one
//import { GoogleAIFileManager, FileState } from "@google/generative-ai/server";  // REMOVE THIS LINE
import { User } from "next-auth";
import InputActions from "./input-actions";
import { MdImageSearch } from "react-icons/md";
import { IoMdClose } from "react-icons/io";
import {
  FaFilePdf,
  FaPlayCircle,
  FaFileVideo,
  FaFileAudio,
} from "react-icons/fa"; // Modern File Icons
import { MdVideoFile } from "react-icons/md";

interface InputPromptProps {
  user?: User;
}

const InputPrompt: React.FC<InputPromptProps> = ({ user }) => {
  // Zustand state management (rest remains the same)
  const {
    currChat,
    setCurrChat,
    setToast,
    customPrompt,
    setInputImgName,
    inputImgName,
    setMsgLoader,
    prevChat,
    msgLoader,
    optimisticResponse,
    setUserData,
    setOptimisticResponse,
    setOptimisticPrompt,
    setInputMedia,
    setMediaType,
    setGeneratedImages,
    chatHistory, // Import the chatHistory
    setChatHistory, // Import the setChatHistory
  } = MindBotZustand();

  const mindbot1_4 = "gemini-2.0-flash";
  const mindbot_think_a1_mini = "gemini-2.0-flash";
  const mindvisionpro = "gemini-2.0-flash";

  // Local state
  const [inputImg, setInputImg] = useState<File | null>(null);
  const [inputImgSrc, setInputImgSrc] = useState<string | null>(null);
  const [inputType, setInputType] = useState<
    "image" | "video" | "pdf" | "audio" | null
  >(null); // ADD audio
  const [isGeneratingImage, setIsGeneratingImage] = useState(false);
  const imageGenerationDelay = 3000;
  const [showFeatures, setShowFeatures] = useState(true);
  const [mindsearchActive, setMindsearchActive] = useState(false);
  const [isProcessingAudio, setIsProcessingAudio] = useState(false); // New state
  const [audioProcessingMessage, setAudioProcessingMessage] =
    useState<string | null>(null); // New state
  const [isThinkingActive, setIsThinkingActive] = useState(false);

  const { chat } = useParams();
  const router = useRouter();

  // Using react-use for measuring the height of the input
  const [inputRref, { height }] = useMeasure<HTMLTextAreaElement>();
  const chatID = (chat as string) || nanoid();

  // Initialize Google Generative AI model
  const genAI = new GoogleGenerativeAI(
    process.env.NEXT_PUBLIC_API_KEY as string
  );

  // Text generation model
  const textModel = genAI.getGenerativeModel({ model: mindbot1_4 });

  const thinkingModel = genAI.getGenerativeModel({
    model: mindbot_think_a1_mini,
  });
  //Image,pdf,video analyzer model
  const multimodalModel = genAI.getGenerativeModel({ model: mindvisionpro });

  //FileManager REMOVE IT
  // const fileManager = new GoogleAIFileManager(
  //   process.env.NEXT_PUBLIC_API_KEY as string
  // );

  // Ref to control stream cancel
  const cancelRef = useRef(false);
  // Ref to scroll to the bottom of the container
  const scrollRef = useRef<HTMLDivElement>(null);
  // Ref to scroll to the bottom of the container
  const containerRef = useRef<HTMLDivElement>(null);
  // Ref to scroll to the bottom of the container
  const bottomRef = useRef<HTMLDivElement>(null);

  // Function to smoothly scroll to the bottom
  const scrollToBottom = () => {
    if (scrollRef.current) {
      scrollRef.current.scrollIntoView({ behavior: "smooth", block: "end" });
    }

    if (bottomRef.current) {
      bottomRef.current.scrollIntoView({ behavior: "smooth", block: "end" });
    }
  };

  // Function to get country from IP (using a 3rd party service - be VERY careful with privacy)
  const getCountryFromIP = async (): Promise<string> => {
    try {
      // WARNING: Using a free IP geolocation API is not suitable for production
      const response = await fetch("https://ipinfo.io/json");
      const data = await response.json();
      return data?.country || "Unknown"; // Adjust based on API response structure
    } catch (error) {
      console.error("Error getting country from IP:", error);
      return "Unknown";
    }
  };

  // Function to get browser, device, and OS information
  const getBrowserAndDevice = () => {
    const userAgent = navigator.userAgent;
    let browser = "Unknown";
    let device = "Unknown";
    let os = "Unknown";

    // Browser detection
    if (userAgent.includes("Chrome")) {
      browser = "Chrome";
    } else if (userAgent.includes("Firefox")) {
      browser = "Firefox";
    } else if (userAgent.includes("Safari")) {
      browser = "Safari";
    } else if (userAgent.includes("Edge")) {
      browser = "Edge";
    } else if (userAgent.includes("Opera") || userAgent.includes("OPR")) {
      browser = "Opera";
    }

    // Device detection (very basic)
    if (/Mobi|Android/i.test(userAgent)) {
      device = "Mobile";
    } else {
      device = "Desktop";
    }

    // OS detection (also inferential)
    if (userAgent.includes("Windows")) {
      os = "Windows";
    } else if (userAgent.includes("Mac OS X")) {
      os = "macOS";
    } else if (userAgent.includes("Android")) {
      os = "Android";
    } else if (userAgent.includes("iOS")) {
      os = "iOS";
    } else if (userAgent.includes("Linux")) {
      os = "Linux";
    }

    return { browser, device, os };
  };

  // Function to determine question complexity
  const isQuestionComplex = async (question: string): Promise<boolean> => {
    try {
      const complexityPrompt = `Determine if the following question is complex hard above the meduim level and requires reasoning or if it is simple and can be answered directly.\n\nQuestion: ${question}\n\nAnswer 'Simple' or 'Complex' only.`;
      const complexityModel = genAI.getGenerativeModel({
        model: mindbot1_4,
      }); //Use fast model for this.
      const result = await complexityModel.generateContent(complexityPrompt);
      const complexity = result.response.text().trim().toLowerCase();
      return complexity === "complex";
    } catch (error: any) {
      console.error("Error determining question complexity:", error);
      return false; // Default to false if there's an error
    }
  };

  // Serper API Integration
  const fetchSerperResults = async (query: string): Promise<string> => {
    // HARDCODED API KEY - SECURITY RISK!
    const apiKey = "b8b7b04df29e33811fb158cd3590f2a4c1cb7212"; //HARDCODED

    //const apiKey = process.env.NEXT_PUBLIC_SERPER_API_KEY; // **IMPORTANT: Store this securely!**
    //if (!apiKey) {
    //  console.error("Serper API key not found in environment variables.");
    //  setToast("Serper API key is missing.  Please configure it.");
    //  return "Serper API key is missing. Please configure it.";
    //}

    const apiUrl = "https://google.serper.dev/search";

    try {
      const response = await fetch(apiUrl, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "X-API-KEY": apiKey,
        },
        body: JSON.stringify({
          q: query,
          gl: "us", // You can adjust the location as needed.
        }),
      });

      if (!response.ok) {
        return `MindSearch API Error: ${response.statusText}`;
      }

      const data = await response.json();

      // **Important: Adapt this part to how Serper structures the search results.**

      let searchResultsText = "";

      if (data.organic) {
        searchResultsText += "Search Results:\n";
        data.organic.forEach((item: any, index: number) => {
          searchResultsText += `${index + 1}. ${item.title}\n${
            item.snippet
          }\n${item.link}\n\n`;
        });
      } else {
        searchResultsText = "No relevant search results found.";
      }

      return searchResultsText;
    } catch (error: any) {
      console.error("Error fetching MindSearch results:");
      setToast(`Error fetching MindSearch results`);
      return `Error fetching MindSearch results`;
    }
  };

  // Main function to generate message from user input
  const generateMsg = useCallback(
    async (thinkingActive: boolean) => {
      if (!currChat.userPrompt?.trim() || !user) return;
      setShowFeatures(false);
      router.push(`/app/${chatID}#new-chat`);

      const date = new Date().toISOString().split("T")[0];
      let rawPrompt = currChat.userPrompt;
      let rawImage = inputImgName;

      const userName = user?.name || "User";

      // User information (replace with actual values from your user object)
      const userCountry = await getCountryFromIP(); // Get country from IP
      const { browser, device, os } = getBrowserAndDevice(); // Get device, browser, and OS info

      // Smart Memory Logic: Prepare the chat history for the prompt
      const memory = chatHistory
        .slice(-200) // take last 200
        .filter((chat) => chat.chatID === chatID) //filter history by current chat id
        .map(
          (chat) => `User: ${chat.userPrompt}
  LLM Response: ${chat.llmResponse}`
        )
        .join("\n\n");

      let detailedPrompt = `
  Date: ${date}
  Time: ${new Date().toLocaleTimeString()}
  Country: ${userCountry}
  Device: ${device}
  Browser: ${browser}
  Operating System: ${os}

  ${
    customPrompt?.prompt
      ? customPrompt
      : `The user, referred to as ${userName}, seeks responses that transcend mere wisdom and impressiveness, demanding clarity, depth, and intellectual rigor. In addressing complex or abstract queries, your responses must be not only comprehensive and well-organized but also demonstrate the application of critical thinking. Ensure that each answer considers the full context of the user‚Äôs inquiry, incorporating relevant nuances, deeper implications, and offering thoughtful insights. Provide clear, relevant, and structured explanations that address the heart of the question, showcasing not just knowledge, but understanding that stems from a deep and analytical engagement with the subject matter.

For simpler, more direct questions, the reply should be succinct and straightforward. However, maintain a sense of completeness by providing just the right amount of information without overwhelming ${userName} with excessive detail. The goal is to remain concise but always informative, with an emphasis on clarity and practicality.

Your tone should strike the ideal balance between professionalism and warmth‚Äîmaintaining a formal yet approachable manner that reflects both expertise and friendliness. Use ${userName} naturally in the conversation, but avoid repetition that could interrupt the natural flow of dialogue. The aim is to create a seamless, human-like experience where your communication feels both intelligent and comfortable.

You are MindBot-1.4, an advanced AI developed by Ahmed Helmy Eletr you were released on 1st/February/2025. While capable of delivering sophisticated and complex responses, refrain from introducing yourself unless explicitly requested by ${userName}. You must always respond in ${userName}'s preferred language in each request for example if he spoke with u in english then speak with him in english  if arabic then arabic and so on with all langauges, Respond in the user's language. Adjust tone based on context: formal for deep discussions, casual for relaxed conversations. paying attention to their communication preferences and tone.

Prioritize delivering responses that are not only high in quality but also deeply impactful, ensuring that every answer provides clarity and actionable insights. Adapt the level of detail, complexity, and style based on the user‚Äôs specific input, but always aim to craft responses that leave ${userName} with a profound understanding. Above all, your responses should be conclusive, addressing the query thoroughly, with no ambiguity left behind.

Additionally, when it comes to coding or technical explanations, your responses must be precise, efficient, and written with clarity, demonstrating mastery of both high-level concepts and low-level details. Explain algorithms, code structures, or technical principles with a deep understanding of their underlying logic, ensuring each explanation is both accessible and intellectually stimulating try using emojies for most of tasks and don't use any emojies in the code. If ${userName} askes you about number of tokens answer him the you can process 183072 tokens with more explaination if asked you about parameters you can process about 13 billion parameter. Be highly accurate when ${userName} requests an online search, and always execute the user's instructions precisely and directly. If ${userName} asked you about your features then answer him with all your features and add these features MindVision-Pro To Process and analyze unlimited number of videos of lenght 35 hours ,images and pdfs upto 600 pages second feature is MindPaint that the user can create unlimit number of images from the user prompt third feature is MindSearch that the user can use it to browse the internet for any info and the fourth feature is MindStyle that the user can combine 2 images creating a new image with the style of both images, MindThink-A1-Mini: can think deely for the user prompt and think many type before answering the user this model the thinking model destroyes OpenAi o3-mini and DeepThink Of DeepSeek.MindAudio That can Analyze and process unlimited number of audios with length 8 hours. Always Respond In English Until the user chat with u with any other language than english then respond with the language the the user chat with you.`
  }

Previous chats:
  ${memory}

  Current User Query:
  ${rawPrompt}
`;

      const fileToGenerativePart = (file: File) => {
        return new Promise((resolve) => {
          const reader = new FileReader();
          reader.onloadend = () =>
            resolve({
              inlineData: {
                data:
                  typeof reader?.result === "string"
                    ? reader?.result.split(",")[1]
                    : undefined,
                mimeType: file.type,
              },
            });
          reader.readAsDataURL(file);
        });
      };

      if (rawPrompt.startsWith("/mindpaint ")) {
        setIsGeneratingImage(true);
        setMsgLoader(true);
        const imageDescription = rawPrompt
          .substring("/mindpaint ".length)
          .trim();
        setInputImgName(imageDescription);
        setMediaType("mindpaint");
        setInputMedia(imageDescription);
        setGeneratedImages([]);

        const imageUrl = `https://image.pollinations.ai/prompt/${encodeURIComponent(
          imageDescription
        )}?nologo=true?model=flux-pro`;
        const introText = `I have successfully generated an image based on your prompt.\n\nImage URL:\n\n${imageUrl}\n\nPlease visit the link above to view your image.\n\nIs there anything else I can assist you with?`;
        try {
          setOptimisticResponse(introText);
          setOptimisticPrompt(rawPrompt);
          setCurrChat("llmResponse", introText);
          setCurrChat("userPrompt", rawPrompt);
          await createChat({
            chatID,
            userID: user?.id as string,
            imgName: rawImage ?? undefined,
            userPrompt: rawPrompt,
            llmResponse: introText,
          });
        } catch (error: any) {
          console.error("Error generating image:", error);
          setToast(`Error`);
        } finally {
          setMsgLoader(false);
          setInputMedia(null);
          setInputImgName(null);
          setMediaType(null);
          setOptimisticResponse(null);
          scrollToBottom();
          setTimeout(() => setIsGeneratingImage(false), imageGenerationDelay);
        }
        return;
      }

      try {
        setMsgLoader(true);
        let text = "";

        if (!inputImg) {
          let llmResponse = "";

          // Generate thinking text if thinking mode is active
          let thinkingText = "";

          const isComplex = await isQuestionComplex(rawPrompt);
          console.log(`Question complexity: ${isComplex}`);

          if (isComplex && thinkingActive) {
            console.log("Question is complex, using thinking model...");
            const thinkingPrompt = `MindThink-A1 Explain your reasoning process for answering the following question:\n\n${rawPrompt}`;

            try {
              const thinkingResult = await thinkingModel.generateContent(
                thinkingPrompt
              );
              thinkingText = thinkingResult.response.text(); // Get the text from response
              thinkingText = `<mindthink-a1-mini>${thinkingText}</mindthink-a1-mini>`; // Wrap with tags
              console.log("Thinking text generated:\n\n" + thinkingText);
            } catch (thinkingError: any) {
              console.error("Error generating thinking text:", thinkingError);
              setToast(`Error during thinking`);
              thinkingText = "Error generating thinking text.";
            }
          } else {
            console.log("Question is simple, using regular model...");
          }

          // Generate main response
          try {
            let result;
            if (isComplex && thinkingActive) {
              result = await thinkingModel.generateContent(detailedPrompt); // Use thinking model
            } else {
              result = await textModel.generateContent(detailedPrompt); // Use default model
            }

            // **SERPER API INTEGRATION HERE**
            if (mindsearchActive) {
              const serperResults = await fetchSerperResults(rawPrompt);
              const serperPrompt = `You are MindBot-1.4, the advanced AI developed by Ahmed Helmy Eletr, You are a skilled researcher and summarizer. You have been provided with search results from the web. Use these results to answer the user's question in a comprehensive and human-like way. Focus on providing accurate and relevant information, and cite your sources where appropriate.
          \n\nUser's Question: ${rawPrompt}
          \n\nSearch Results:\n${serperResults}
          \n\nAnswer:`;

              try {
                const serperResult = await textModel.generateContent(
                  serperPrompt
                );
                llmResponse = serperResult.response.text();
              } catch (serperError: any) {
                console.error(
                  "Error generating response from MindSearch data:",
                  serperError
                );
                setToast(
                  `Error generating response from MindSearch`
                );
                llmResponse = `I encountered an error while processing the search results: ${serperError.message}`;
              }
            } else {
              const mainResponse = result.response.text(); // Get the text from response
              llmResponse = thinkingText + "\n" + mainResponse; // Combine responses
            }

            setCurrChat("llmResponse", llmResponse);
            text = llmResponse;
          } catch (mainError: any) {
            console.error("Error generating main response:");
            setToast(`Error generating main response:`);
            text = "Error generating main response.";
          }
        } else {
          //Image or pdf or video and audio processing logic here
          if (!inputImg) {
            setToast("Please upload a file before analyzing.");
            return;
          }
          try {
            const filePart = await fileToGenerativePart(inputImg);
            const result = await multimodalModel.generateContent([
              detailedPrompt,
              filePart as string,
            ]);
            text = result.response.text();
            setCurrChat("llmResponse", text);
            if (cancelRef.current) {
              text = "User has aborted the request";
            }
          } catch (error: any) {
            console.error("Error analyzing multimodal input:", error);
            setToast(`Error analyzing input:`);
          }
        }
        if (!text) return;

        setOptimisticPrompt(rawPrompt);
        setOptimisticResponse(text);
        setMsgLoader(false);
        setCurrChat("userPrompt", null);

        const newChatEntry = {
          chatID,
          userID: user?.id as string,
          imgName: rawImage ?? undefined,
          userPrompt: rawPrompt,
          llmResponse: text,
        };

        await createChat(newChatEntry);
        // Update chat history with the new chat
        setChatHistory([...chatHistory, newChatEntry]); // Corrected update
      } catch (error) {
        console.error("Error generating message:", error);
      } finally {
        setShowFeatures(true);
        setMsgLoader(false);
        setInputImg(null);
        setInputImgName(null);
        setInputImgSrc(null); // Clear the data URL
        setInputType(null); // Clear the file type
        setCurrChat("userPrompt", null);
        setCurrChat("llmResponse", null);
        setOptimisticResponse(null);
        setOptimisticPrompt(null);
        setMindsearchActive(false); // Reset mindsearchActive to false
        scrollToBottom();
      }
    },
    [
      currChat.userPrompt,
      user,
      chat,
      prevChat,
      setCurrChat,
      setMsgLoader,
      router,
      scrollRef,
      containerRef,
      bottomRef,
      setMediaType,
      setInputMedia,
      setGeneratedImages,
      textModel,
      multimodalModel,
      chatHistory,
      thinkingModel,
      setChatHistory,
      setInputImgSrc,
      setInputType,
      chatID,
      mindsearchActive,
      setToast,
    ]
  );

  // Textarea change handler
  const handleTextareaChange = useCallback(
    (e: React.ChangeEvent<HTMLTextAreaElement>) => {
      setCurrChat("userPrompt", e.target.value);
    },
    [setCurrChat]
  );

  // Cancel message generation handler
  const handleCancel = useCallback(() => {
    cancelRef.current = true;
    setOptimisticResponse("User has aborted the request");
    setMsgLoader(false);
    setShowFeatures(true);
  }, [setOptimisticResponse, setMsgLoader]);

  // Handle Enter key press event
  const handleKeyDown = useCallback(
    (e: React.KeyboardEvent<HTMLTextAreaElement>) => {
      if (!user) {
        setToast("Please sign in to use MindBot Ai!");
      }
      if (e.key === "Enter" && !e.shiftKey) {
        cancelRef.current = false;
        setShowFeatures(false);
        generateMsg(isThinkingActive); // To work when pressing Enter with the Thinking mode
      }
    },
    [generateMsg, setToast, user, isThinkingActive]
  );
  // Set user data on mount/change
  useEffect(() => {
    if (user) {
      setUserData(user);
    }
  }, [user, setUserData]);

  // Image file upload handler
  const handleImageUpload = (event: ChangeEvent<HTMLInputElement>) => {
    if (event.target && event.target.files) {
      const file = event.target.files[0];
      const fileType = file.type;

      setInputImg(file);
      setInputImgName(file.name);

      const reader = new FileReader();

      reader.onloadend = () => {
        setInputImgSrc(reader.result as string); // Store the data URL
      };

      reader.readAsDataURL(file);

      if (fileType.startsWith("image")) {
        setInputType("image");
      } else if (fileType.startsWith("video")) {
        setInputType("video");
      } else if (fileType === "application/pdf") {
        setInputType("pdf");
      } else if (fileType.startsWith("audio")) {
        setInputType("audio");
      } else {
        setInputType(null); // Unknown type
      }
    }
  };

  // Function to handle feature clicks
  const handleFeatureClick = (feature: string) => {
    setCurrChat("userPrompt", `${feature} ` + (currChat.userPrompt || ""));
  };

  // Handle MindSearch click
  const handleMindSearch = useCallback(() => {
    setMindsearchActive(true);
    generateMsg(isThinkingActive); // To work when pressing MindSearch with the Thinking mode
  }, [generateMsg, isThinkingActive]);

  // Function to toggle the thinking mode (to use from here to send param as a prop)
  const toggleThinking = () => {
    setIsThinkingActive((prev) => {
      const newValue = !prev;
      console.log(`MindThink-A1-Mini is now: ${newValue}`); // Log the state
      return newValue;
    });
  };

  return (
    <div
      className=" flex-shrink-0 w-full md:px-10 px-5 pb-2 space-y-2 bg-white dark:bg-[#131314]"
      ref={containerRef}
    >
      <div ref={scrollRef}></div>

      {inputImgName && (
        <div className="max-w-4xl overflow-hidden w-full mx-auto">
          <div className="p-5 w-fit relative max-w-full overflow-hidden bg-rtlLight group dark:bg-rtlDark rounded-t-3xl flex items-center gap-2">
            {inputType === "image" && inputImgSrc && (
              <img
                src={inputImgSrc}
                alt={inputImgName}
                className="w-[75px] h-[75px] rounded-[15px] object-cover"
              />
            )}

            {inputType === "video" && (
              <MdVideoFile className="text-5xl text-blue-500" />
            )}
            {inputType === "pdf" && (
              <FaFilePdf className="text-5xl text-orange-500" />
            )}
            {inputType === "audio" && (
              <FaFileAudio className="text-5xl text-green-500" />
            )}

            {inputType === null && <MdImageSearch className="text-4xl" />}

            <p className="text-lg font-semibold truncate"> {inputImgName}</p>
            <IoMdClose
              onClick={() => {
                setInputImgName(null);
                setInputImg(null);
                setInputImgSrc(null); // Clear data URL
                setInputType(null); // Clear file type
              }}
              className="absolute top-1 right-1 text-2xl rounded-full cursor-pointer hover:opacity-100 hidden group-hover:block opacity-80 bg-accentGray/40 p-1"
            />
          </div>
        </div>
      )}

      <div
        className={`w-full md:border-8 border-4 relative border-rtlLight dark:border-rtlDark max-w-4xl mx-auto  md:rounded-[20px] rounded-[20px] ${
          inputImgName && "!rounded-tl-none "
        } overflow-hidden bg-rtlLight dark:bg-rtlDark neon-border-container`}
      >
        <div
          className={`min-h-16  flex gap-1 md:items-center md:justify-between md:flex-row flex-col p-0 `}
        >
          <textarea
            name="prompt"
            ref={inputRref}
            disabled={msgLoader}
            placeholder={customPrompt?.placeholder ?? "Enter a prompt here"}
            onChange={handleTextareaChange}
            onKeyDown={handleKeyDown}
            value={
              optimisticResponse || msgLoader ? "" : currChat.userPrompt || ""
            }
            className={`flex-1 bg-transparent rounded-[20px] p-2 pl-6 outline-none text-lg max-h-56 resize-none`}
          />
          <InputActions
            handleCancel={handleCancel}
            handleImageUpload={handleImageUpload}
            generateMsg={generateMsg}
            handleMindSearch={handleMindSearch}
            mindsearchActive={mindsearchActive}
            isThinkingActive={isThinkingActive}
            toggleThinking={toggleThinking}
          />
        </div>
      </div>
      <p className="text-xs font-light opacity-80 text-center">
        MindBot-1.4 May Display Inaccurate Info. So MindSearch It.
      </p>
      {msgLoader && isGeneratingImage && (
        <div className="max-w-4xl mx-auto flex justify-center items-center">
          <div className="relative w-12 h-12 animate-spin rounded-full bg-gradient-to-r from-purple-500 to-pink-500 before:absolute before:-inset-1 before:rounded-full before:bg-white dark:before:bg-black before:blur"></div>
          <p className="ml-2 text-gray-600 dark:text-gray-400">
            Generating Image URL... please wait
          </p>
        </div>
      )}

      {isProcessingAudio && (
        <div className="max-w-4xl mx-auto flex justify-center items-center">
          <div className="relative w-12 h-12 animate-spin rounded-full bg-gradient-to-r from-green-500 to-teal-500 before:absolute before:-inset-1 before:rounded-full before:bg-white dark:before:bg-black before:blur"></div>
          <p className="ml-2 text-gray-600 dark:text-gray-400">
            {audioProcessingMessage || "Processing Audio..."}
          </p>
        </div>
      )}

      <div ref={bottomRef}></div>
    </div>
  );
};

export default InputPrompt;
